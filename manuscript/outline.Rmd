---
title: "Outline"
author: "William Kumler"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = '..')
options(readr.show_col_types = FALSE)
options(tidyverse.quiet = TRUE)
```

```{r load data}
library(tidyverse)

file_data <- read_csv("made_data_MS3000/file_data.csv") %>%
  mutate(depth=factor(depth, levels=c("15m", "DCM", "175m"))) %>%
  mutate(filename=basename(filename))
FT2040_features <- read_csv("made_data_FT2040/features_extracted.csv")
MS3000_features <- read_csv("made_data_MS3000/features_extracted.csv")
Pttime_features <- read_csv("made_data_Pttime/features_extracted.csv")
CultureData_features <- read_csv("made_data_CultureData/features_extracted.csv")
```

## Abstract

## Introduction

LC/MS techniques are neat

  - Fast sample processing
  - Low detection limits
  - Can characterize new molecules

Most of the data produced by them goes unused

  - Targeted workflows look at only a fraction of the data
  - Untargeted algorithms are still fairly new

Peakpicking is hard

  - Smooth transition from "good" into "meh" into "bad" across the limit of detection
  - Fundamental tradeoff between false positives and false negatives
  - Lots of false positive peaks because attempting to minimize false negative
  - Requires manual curation of peak quality which doesn't scale

Algorithms don't provide likelihood of "goodness"

  - Typical output: m/z, rt, area, maybe SN
  - We want/need a probability that this peak is just noise
  - Want to control the false positive/negative rate depending on project goals
  - We don't need new algorithms as much as we need a way to quantify success on the existing ones

Multiple advantages to a robust assessment of peak quality

  - Reduces manual labor in sorting out good peaks from bad
  - Improves statistical power
    - Reduces the number of hypotheses tested for univariate analyses
    - Increases relative power of significant peaks for multivariate analyses
  - Allows optimization of chromatography for untargeted approaches
    - More peaks isn't better if the additional peaks are just noise
  - Allows optimization of peakpicking parameters for untargeted approaches
  - "Good" peaks should be independent of software choices while noise is noisy
  - Enables more consistent quality control across labs
  - Focuses calculation time/energy on best peaks rather than all equally

A single comprehensive quality parameter is more useful than many independent ones

  - Enables sensible threshold selection (50% likely to be a peak)
    - Rather than current arbitrary approaches (SNR > 20, peak height > 10e7)
  - Single metric has multivariate advantage over multiple univariate thresholds
    - Good peak can fall be weak in one or two metrics and still be safe
  - Relative power of different metrics has never been computed/compared
    - More important to have an isotope? Large area? Good shape? Differ between treatments?

Can be used to check improvements in peakpicking/chromatography

  - Traditional method: a few random peaks are checked after making a change to params
  - Total number of peaks used as proxy, not total number of GOOD peaks
  - Did this change improve overall peak quality?
  - Currently impossible to optimize these for untargeted peakpicking



## Methods

### Sample collection

### Sample processing

### LC conditions

### MS conditions

### Peakpicking with XCMS

### Manual inspection and classification
Gold standard: manual inspection by a human expert

  - Decision to view the entire feature rather than a single trace
  - Decision to color by treatments

```{r fig:some good some bad features}

```

### Feature extraction
Feature extraction - many possible metrics, calculated across features

  - Some trivial to calculate
    - Mean m/z, mean RT, number found
  - Some require access to the raw data
    - Peak shape, peak SNR, isotope existence, number of missed scans

```{r how are raw peak shape metrics calculated}

```

## Results

```{r colored histograms on all 4 datasets}
# Train on MESOSCOPE/Falkor combined model
# Raw metrics only
# Color by feature class
both_min_model <- rbind(FT2040_features, MS3000_features) %>%
  select(feat_class, med_cor, med_SNR) %>%
  filter(feat_class%in%c("Good", "Bad")) %>%
  mutate(feat_class=feat_class=="Good") %>%
  glm(formula=feat_class~., family = binomial)
bind_rows(lst(
  Falkor=FT2040_features, MESOSCOPE=MS3000_features,
  CultureData=CultureData_features, Pttime=Pttime_features), 
  .id = "dataset") %>%
  mutate(pred_prob=predict(object=both_min_model,newdata = ., type = "response")) %>%
  mutate(pred_class=ifelse(pred_prob>0.9, "Good", "Bad")) %>%
  mutate(error_type=case_when(
    feat_class=="Good" & pred_class=="Good" ~ "%TP",
    feat_class=="Good" & pred_class=="Bad" ~ "%FN",
    feat_class=="Bad" & pred_class=="Good" ~ "%FP",
    feat_class=="Bad" & pred_class=="Bad" ~ "%TN",
    TRUE ~ "Incomparable"
  )) %>%
  mutate(dataset=factor(dataset, levels=c("MESOSCOPE", "Falkor", "CultureData", "Pttime"))) %>%
  ggplot() +
  geom_histogram(aes(x=pred_prob, fill=feat_class), breaks=seq(0, 1, 0.02)) +
  facet_wrap(~dataset, ncol = 1, scales="free_y") +
  coord_cartesian(ylim = c(0, 200)) +
  geom_vline(xintercept = 0.9, color="black", linewidth=1) +
  scale_fill_manual(breaks = c("Good", "Bad", "Meh", "Stans only", "Unclassified"),
                    values = c("#f4bb23", "#a41118", "#0b505c", "#028e34", "grey50"))
# Add broken y-axis
# Fix axis titles
# Format nicely
```

```{r FDR vs GPF cross-train triangle figure feature selection}
# Dark/light green/blue triangle plot
# Include best-fit lines for easier slope comparison
# but don't extend beyond data
dataset_versions <- c("FT2040_features", "MS3000_features")

raw_data_params <- c("med_cor", "med_SNR")
xcms_params <- c("mean_mz", "sd_ppm", "mean_rt", "sd_rt", "mean_pw", 
                 "sd_pw", "log_mean_area", "log_sd_area", "sn", 
                 "f", "scale", "lmin", "feat_npeaks", "n_found", 
                 "samps_found", "stans_found")
all_params <- c("mean_mz", "sd_ppm", "mean_rt", "sd_rt", "mean_pw", "sd_pw",
                "log_mean_area", "log_sd_area", "sn", "f", "scale", 
                "lmin", "feat_npeaks", "n_found", "samps_found", "stans_found", 
                "med_cor", "med_SNR", "med_missed_scans", "smp_to_blk", 
                "smp_to_std", "shape_cor", "area_cor", "feat_class")

lst(all_params, raw_data_params, xcms_params) %>%
  imap(function(param_selection, param_name){
    lapply(dataset_versions, function(train_set){
      full_model <- get(train_set) %>% 
        filter(feat_class%in%c("Good", "Bad")) %>%
        select(feat_class, all_of(param_selection)) %>%
        mutate(feat_class=feat_class=="Good") %>%
        glm(formula=feat_class~., family = binomial)
      lapply(dataset_versions, function(test_set){
        get(test_set) %>%
          mutate(pred_prob=predict(object=full_model,newdata = ., type = "response")) %>%
          mutate(pred_class=ifelse(pred_prob>0.5, "Good", "Bad")) %>%
          select(feat_class, starts_with("pred_class")) %>%
          mutate(train_test=paste(train_set, test_set, sep = "-")) %>%
          mutate(train_test=str_remove_all(train_test, "_features")) %>%
          mutate(cross_type=ifelse(train_set==test_set, "Same", "Different"))
      }) %>% bind_rows()
    }) %>% bind_rows() %>% mutate(which_params=param_name)
  }) %>% bind_rows() %>%
  mutate(error_type=case_when(
    feat_class=="Bad" & pred_class=="Bad" ~ "TN",
    feat_class=="Bad" & pred_class=="Good" ~ "FP",
    feat_class=="Good" & pred_class=="Bad" ~ "FN",
    feat_class=="Good" & pred_class=="Good" ~ "TP"
  )) %>%
  group_by(train_test, which_params, cross_type) %>%
  count(error_type) %>%
  ungroup() %>%
  mutate(cross_type=factor(cross_type, levels=c("Same", "Different"))) %>%
  pivot_wider(names_from=error_type, values_from = n) %>%
  mutate(FDR=round(FP/(FP+TP)*100)) %>%
  mutate(GPF=round(TP/(FN+TP)*100)) %>%
  pivot_longer(c(FDR, GPF)) %>%
  mutate(name=paste0("%", name)) %>%
  mutate(which_params=factor(which_params, 
                             labels=c("All\nparams", "Default\nonly", "Raw\ndata params"),
                             levels=c("all_params", "xcms_params", "raw_data_params"))) %>%
  mutate(cross_type_num=as.numeric(cross_type)) %>%
  ggplot() +
  geom_point(aes(x=cross_type, y=value, color=train_test, fill=train_test, shape=name)) +
  geom_smooth(aes(x=cross_type_num, y=value, group=name), 
              method = "lm", formula="y~x", color="black", se=FALSE) +
  facet_grid(~which_params) +
  scale_x_discrete(labels = c("Same\ndataset\n", "Different\ndataset\n"),
                   breaks = c("Same", "Different")) +
  scale_color_manual(values = c("#0b505c", "#0d8299", "#184d1f", "#028e34"), 
                     breaks = c("MS3000-MS3000", "MS3000-FT2040", "FT2040-FT2040", "FT2040-MS3000"),
                     aesthetics = c("color", "fill")) +
  scale_shape_manual(breaks = c("%FDR", "%GPF"), values = c(25, 17)) +
  coord_cartesian(ylim = c(0, 100)) +
  theme_bw()
```

```{r red/gold rank dotplot step-down method}
# Make sure to include overplotting somehow?
dataset_versions <- c("FT2040_features", "MS3000_features")
raw_data_params <- c("med_cor", "med_SNR")
all_params <- c("mean_mz", "sd_ppm", "mean_rt", "sd_rt", "mean_pw", "sd_pw",
                "log_mean_area", "log_sd_area", "sn", "f", "scale", 
                "lmin", "feat_npeaks", "n_found", "samps_found", "stans_found", 
                "med_cor", "med_SNR", "med_missed_scans", "smp_to_blk", 
                "smp_to_std", "shape_cor", "area_cor", "feat_class")
dotplot_stepdown_data <- lst(all_params, raw_data_params) %>%
  imap(function(param_selection, param_name){
    lapply(dataset_versions, function(train_set){
      model_i <- get(train_set) %>% 
        filter(feat_class%in%c("Good", "Bad")) %>%
        select(feat_class, all_of(param_selection)) %>%
        mutate(feat_class=feat_class=="Good") %>%
        glm(formula=feat_class~., family = binomial)
      MS3000_features %>%
        mutate(pred_prob=predict(model_i, ., type="response")) %>%
        select(feature, feat_class, pred_prob) %>%
        mutate(which_params=param_name) %>%
        mutate(training_set=train_set)
    }) %>% bind_rows()
  }) %>% bind_rows()
raw_prob_space_gp <- dotplot_stepdown_data %>%
  pivot_wider(names_from = training_set, values_from = pred_prob) %>%
  filter(feat_class%in%c("Good", "Bad")) %>%
  ggplot() +
  geom_abline(slope = 1, intercept = 0, color="black", lwd=1) +
  geom_point(aes(x=FT2040_features, y=MS3000_features, color=feat_class)) +
  facet_wrap(~which_params, nrow = 1) +
  scale_fill_manual(values = c("#a41118", "#f4bb23"), breaks = c("Bad", "Good"), 
                    aesthetics = c("color", "fill")) +
  scale_x_continuous(breaks = c(0, 0.5, 1)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  labs(x="Falkor-trained model", y="MESO-trained model")
rank_prob_space_gp <- dotplot_stepdown_data %>%
  group_by(training_set, which_params) %>%
  mutate(pred_prob=rank(pred_prob)) %>%
  pivot_wider(names_from = training_set, values_from = pred_prob) %>%
  filter(feat_class%in%c("Good", "Bad")) %>%
  ggplot() +
  geom_abline(slope = 1, intercept = 0, color="black", lwd=1) +
  geom_point(aes(x=FT2040_features, y=MS3000_features, color=feat_class)) +
  facet_wrap(~which_params, nrow = 1) +
  scale_fill_manual(values = c("#a41118", "#f4bb23"), breaks = c("Bad", "Good"), 
                    aesthetics = c("color", "fill")) +
  scale_x_continuous(breaks = c(0, nrow(MS3000_features)), 
                     labels = c("Min rank", "Max rank")) +
  scale_y_continuous(breaks = c(0, nrow(MS3000_features)),
                     labels = c("Min\nrank", "Max\nrank")) +
  labs(x="Falkor-trained model", y="MESO-trained model") +
  theme(axis.text.x = element_text(hjust=c(0, 1)))
plot(gridExtra::arrangeGrob(raw_prob_space_gp, rank_prob_space_gp))
```

```{r FDR vs GPF with additional datasets}
# Currently a fairly simple figure, may need additional data?
both_min_model <- rbind(FT2040_features, MS3000_features) %>%
  select(feat_class, med_cor, med_SNR) %>%
  filter(feat_class%in%c("Good", "Bad")) %>%
  mutate(feat_class=feat_class=="Good") %>%
  glm(formula=feat_class~., family = binomial)
pred_probs_all <- bind_rows(lst(
  Falkor=FT2040_features, MESOSCOPE=MS3000_features,
  CultureData=CultureData_features, Pttime=Pttime_features), 
  .id = "dataset") %>%
  mutate(pred_prob=predict(object=both_min_model,newdata = ., type = "response")) %>%
  mutate(pred_class=ifelse(pred_prob>0.9, "Good", "Bad")) %>%
  mutate(error_type=case_when(
    feat_class=="Good" & pred_class=="Good" ~ "%TP",
    feat_class=="Good" & pred_class=="Bad" ~ "%FN",
    feat_class=="Bad" & pred_class=="Good" ~ "%FP",
    feat_class=="Bad" & pred_class=="Bad" ~ "%TN",
    TRUE ~ "Incomparable"
  )) %>%
  mutate(dataset=factor(dataset, levels=c("MESOSCOPE", "Falkor", "CultureData", "Pttime"))) %>%
  filter(error_type!="Incomparable") %>%
  group_by(dataset, error_type) %>%
  summarise(n=n(), .groups = 'drop') %>%
  complete(dataset, error_type, fill=list(n=0))
pred_probs_all %>%
  pivot_wider(names_from=error_type, values_from = n) %>%
  mutate(`%FDR`=`%FP`/(`%FP`+`%TP`)*100) %>%
  mutate(`%GPF`=`%TP`/(`%FN`+`%TP`)*100) %>%
  mutate(`%GPF`=ifelse(dataset%in%c("CultureData", "Pttime"), NA, `%GPF`)) %>%
  pivot_longer(c(`%FDR`, `%GPF`)) %>%
  mutate(desc=case_when(
    is.na(value) ~ NA,
    name=="%FDR"~ paste0("frac(",`%FP`,",",`%FP`+`%TP`,")"),
    name=="%GPF"~ paste0("frac(",`%TP`,",",`%FN`+`%TP`,")")
  )) %>%
  mutate(vjust_custom=case_when(
    is.na(value) ~ NA,
    name=="%FDR" & dataset=="Pttime" ~ 1.3,
    name=="%FDR" ~ -0.3,
    name=="%GPF" & dataset=="Falkor" ~ 1.3,
    name=="%GPF" ~ -0.3,
  )) %>%
  drop_na() %>% 
  mutate(name=factor(name, levels=c("%GPF", "%FDR"))) %>%
  mutate(dataset=factor(dataset, levels = c("Falkor", "MESOSCOPE", "CultureData", "Pttime"))) %>%
  ggplot(aes(x=dataset, y=value)) +
  geom_point(size=3) +
  geom_text(aes(label=desc, vjust=vjust_custom), parse=TRUE) +
  facet_wrap(~name, scales = "free_y", ncol = 1) +
  scale_y_continuous(limits = c(0, NA))

# Confusion matrices for confirmation
# pred_probs_all
# pred_probs_all %>%
#   ggplot(aes(x=error_type, y=n)) +
#   geom_col(color="#0b505c", fill="#0b505c") +
#   geom_text(aes(label=n), vjust=-0.2, color="#0b505c") +
#   facet_wrap(~dataset, nrow=1) +
#   theme_bw() +
#   theme(axis.title = element_blank())
```

```{r clockplot BMIS}
if(!file.exists("manuscript/MS3000_norm_areas.csv")){
  source("manuscript/BMIS_peakprep.R")
}
```

```{r clockplot dataset munging}
norm_area_data <- read_csv("manuscript/MS3000_norm_areas.csv")

both_min_model <- rbind(FT2040_features, MS3000_features) %>%
  select(feat_class, med_cor, med_SNR) %>%
  filter(feat_class%in%c("Good", "Bad")) %>%
  mutate(feat_class=feat_class=="Good") %>%
  glm(formula=feat_class~., family = binomial)
best_feats <- MS3000_features %>%
  filter(feat_class%in%c("Good", "Bad")) %>%
  mutate(pred_prob=predict(object=both_min_model,newdata = ., type = "response")) %>%
  mutate(pred_class=ifelse(pred_prob>0.5, "Good", "Bad")) %>%
  select(feature, feat_class, pred_class)
clockplot_data <- norm_area_data %>%
  right_join(best_feats, by = join_by(feature)) %>%
  left_join(file_data %>% select(filename, depth), by = join_by(filename))
```

```{r clockplot kw_dunn_tests}
kw_pvals_all <- clockplot_data %>%
  nest(data=-c(feature, pred_class)) %>%
  mutate(tidyaov=map(
    data, ~ broom::tidy(kruskal.test(.x$norm_area~.x$depth)))
  ) %>%
  unnest(tidyaov) %>%
  select(-parameter, -method) %>%
  mutate(cor_pval_all=p.adjust(p.value, method="fdr")) %>%
  group_by(pred_class) %>%
  mutate(cor_pval_byclass=p.adjust(p.value, method="fdr")) %>%
  ungroup() %>%
  select(feature, cor_pval_byclass)
resp_class_levels <- c("All equal", 
    "15m > DCM = 175m", "15m > DCM > 175m", "15m = DCM > 175m",
    "DCM > 15m > 175m", "DCM > 15m = 175m", "DCM > 175m > 15m", 
    "15m < DCM = 175m", "15m < DCM < 175m", "15m = DCM < 175m", 
    "DCM < 15m < 175m", "DCM < 15m = 175m", "DCM < 175m < 15m", 
    "Hard to tell")
kw_classes_all <- clockplot_data %>%
  group_by(feature) %>%
  mutate(norm_area=rank(norm_area)) %>%
  filter(var(norm_area)>0) %>%
  rstatix::dunn_test(norm_area~depth, p.adjust.method = "none") %>%
  mutate(contrast=paste(group2, group1, sep = "_")) %>%
  select(feature, contrast, est=statistic, pval=p.adj) %>%
  pivot_wider(names_from = contrast, values_from = c(pval, est)) %>%
  mutate(resp_class=case_when(
    pval_DCM_15m<0.05 & pval_175m_15m<0.05 & pval_175m_DCM<0.05 & est_DCM_15m<0 & est_175m_15m<0 & est_175m_DCM<0 ~ "15m > DCM > 175m",
    pval_DCM_15m<0.05 & pval_175m_15m<0.05 & pval_175m_DCM<0.05 & est_DCM_15m>0 & est_175m_15m<0 & est_175m_DCM<0 ~ "DCM > 15m > 175m",
    pval_DCM_15m<0.05 & pval_175m_15m<0.05 & pval_175m_DCM<0.05 & est_DCM_15m<0 & est_175m_15m>0 & est_175m_DCM>0 ~ "DCM < 15m < 175m",
    pval_DCM_15m<0.05 & pval_175m_15m<0.05 & pval_175m_DCM<0.05 & est_DCM_15m>0 & est_175m_15m>0 & est_175m_DCM>0 ~ "15m < DCM < 175m",
    pval_DCM_15m<0.05 & pval_175m_15m<0.05 & pval_175m_DCM<0.05 & est_DCM_15m>0 & est_175m_15m>0 & est_175m_DCM<0 ~ "DCM > 175m > 15m",
    pval_DCM_15m<0.05 & pval_175m_15m<0.05 & pval_175m_DCM<0.05 & est_DCM_15m<0 & est_175m_15m<0 & est_175m_DCM>0 ~ "DCM < 175m < 15m",
    pval_DCM_15m<0.05 & pval_175m_15m<0.05 & pval_175m_DCM>0.05 & est_DCM_15m<0 & est_175m_15m<0 ~ "15m > DCM = 175m",
    pval_DCM_15m<0.05 & pval_175m_15m<0.05 & pval_175m_DCM>0.05 & est_DCM_15m>0 & est_175m_15m>0 ~ "15m < DCM = 175m",
    pval_DCM_15m>0.05 & pval_175m_15m<0.05 & pval_175m_DCM<0.05 & est_175m_15m<0 & est_175m_DCM<0 ~ "15m = DCM > 175m",
    pval_DCM_15m>0.05 & pval_175m_15m<0.05 & pval_175m_DCM<0.05 & est_175m_15m>0 & est_175m_DCM>0 ~ "15m = DCM < 175m",
    pval_DCM_15m<0.05 & pval_175m_15m>0.05 & pval_175m_DCM<0.05 & est_DCM_15m>0 & est_175m_DCM<0 ~ "DCM > 15m = 175m",
    pval_DCM_15m<0.05 & pval_175m_15m>0.05 & pval_175m_DCM<0.05 & est_DCM_15m<0 & est_175m_DCM>0 ~ "DCM < 15m = 175m",
    pval_DCM_15m>0.05 & pval_175m_15m>0.05 & pval_175m_DCM>0.05 ~ "All equal",
    TRUE ~ "Hard to tell"
  )) %>%
  select(feature, resp_class) %>%
  mutate(resp_class=factor(resp_class, levels=resp_class_levels))

clockplot_xy_spots <- clockplot_data %>%
  group_by(feature) %>%
  mutate(norm_area=rank(norm_area)) %>%
  group_by(feature, depth) %>%
  summarise(mean_fold_change=mean(norm_area/50), .groups = "drop") %>% 
  # div by 50 bc 99 files and median 1:99 is 50
  pivot_wider(names_from=depth, values_from=mean_fold_change) %>%
  mutate(x=DCM-1) %>%           # cursed but works??
  mutate(y=`15m`+DCM/2-1.5) %>% # super fucking cursed
  select(feature, x, y)

clockplot_complete_df <- clockplot_data %>%
  distinct(feature, feat_class, pred_class) %>%
  left_join(clockplot_xy_spots, by = join_by(feature)) %>%
  left_join(kw_pvals_all, by = join_by(feature)) %>%
  left_join(kw_classes_all, by = join_by(feature)) %>%
  mutate(error_type=case_when(
    pred_class=="Good" & feat_class=="Good" ~ "TP",
    pred_class=="Good" & feat_class=="Bad" ~ "FP",
    pred_class=="Bad" & feat_class=="Good" ~ "FN",
    pred_class=="Bad" & feat_class=="Bad" ~ "TN"
  )) %>%
  mutate(error_type=factor(error_type, levels=c("TP", "FP", "FN", "TN"),
                           labels=c("Good peak found", "Bad peak included",
                                    "Good peak missed", "Bad peak avoided")))
```

```{r clockplot create circular colormap}
cmap_name <- "infinity"
cmap_url <- paste0("https://raw.githubusercontent.com/1313e/CMasher/master/",
                   "cmasher/colormaps/", cmap_name, "/", cmap_name, "_norm.txt")
cmtable <- read.table(cmap_url)
cmscale <- rgb(cmtable[,1], cmtable[,2], cmtable[,3])
color_scale <- c("grey50", head(cmscale[seq(1, length(cmscale), length.out=13)], 12), "grey80")
names(color_scale) <- resp_class_levels
```

```{r clockplot}
clock_scale <- 0.66
axis_xends <- c(-1/2, 1.1, -1/2)*clock_scale
axis_yends <- c(sqrt(3)/2, 0, -sqrt(3)/2)*clock_scale
axis_y_dodge <- c(2, 2, -2)*clock_scale/25
clockplot_base_gp <- ggplot() +
  annotate("segment", x=c(0,0,0), y=c(0,0,0), 
           xend=axis_xends, yend=axis_yends,
           arrow=arrow(length=unit(0.02, "npc"), type = "closed")) +
  annotate("segment", x=c(0,0,0), y=c(0,0,0),
           xend=-axis_xends, yend=-axis_yends,
           lty = 2) +
  annotate("label", x=axis_xends, y=axis_yends+axis_y_dodge,
           label=c("15m axis", "DCM axis", "175m axis"), 
           fill="#FFFFFFAA", label.size=0) +
  coord_fixed(xlim = c(-0.65, 1.3)*clock_scale, 
              ylim = c(-1.2, 1.2)*clock_scale, 
              expand=TRUE) +
  scale_color_manual(values = color_scale, breaks = names(color_scale), drop=FALSE) +
  theme_void() +
  theme(legend.position = "right", legend.title = element_blank()) +
  guides(colour = guide_legend(override.aes = list(size=4))) +
  aes(x=x, y=y, color=resp_class, label=feature)

# clockplot_base_gp +
#   geom_point(alpha=0.8, data = clockplot_complete_df)

clockplot_base_gp +
  geom_point(aes(shape=error_type), alpha=0.8, size=2,
             data = clockplot_complete_df) +
  scale_shape_manual(breaks = c("Good peak found", "Bad peak included",
                                "Good peak missed", "Bad peak avoided"), 
                     values = c(19, 3, 4, 1))
```

```{r manual check on clockplot, eval=FALSE}
library(RaMS)
# msdata <- readRDS("made_data_MS3000/msdata.rds")
feat_i <- "FT0828"
row_data <- MS3000_features[MS3000_features$feature==feat_i,]
eic_gp <- msdata$EIC2[mz%between%pmppm(row_data$mean_mz, 5)] %>%
  .[rt%between%(row_data$mean_rt/60+c(-1, 1))] %>%
  .[str_detect(filename, "_Smp_")] %>%
  .[str_detect(filename, "180821")] %>%
  left_join(file_data) %>%
  ggplot() +
  geom_line(aes(x=rt, y=int, group=filename, color=depth))
bxp_gp <- peak_data %>% 
  select(feature, filename, into) %>%
  filter(feature==feat_i) %>%
  filter(str_detect(filename, "_Smp_")) %>%
  filter(str_detect(filename, "180821")) %>%
  left_join(file_data, by = join_by(filename)) %>%
  ggplot() +
  geom_boxplot(aes(x=depth, y=into))
plot(gridExtra::arrangeGrob(eic_gp, bxp_gp))
```


## Discussion

## Citations

2020 review and demand for more robust peakpicking and description of noise sources
https://www.sciencedirect.com/science/article/pii/S0165993620302922

2014 Comparative evaluation of preprocessing freeware on chromatography/mass spectrometry data for signature discovery
https://www.sciencedirect.com/science/article/pii/S0021967314010450?via%3Dihub
  - XCMS is the most cited preprocessing tool currently in the metabolomics literature
  - Table comparing parameters across XCMS, MzMine, MetAlign
  - Selection by appearance in multiple replicates
  - Visual inspection of peaks for quality
  - Reports percentage of false positives as between 90% and 50%
  - Most pressing improvement needed for all the tested data analysis tools was to reduce the percentage of false peaks

2003 the multivariate advantage
https://www.sciencedirect.com/science/article/pii/S0003267003006810?via%3Dihub

2017 One Step Forward for Reducing False Positive and False Negative Compound Identifications



## Unstructured thoughts
Pleas:
  - Report peak boundaries! If not max/min m/z then at least max/min RT
    - Helps with visualization and raw data feature extraction
  - Run a pooled/aggregate sample

Brags:
  - Built the classifier tool in R, very fast, uses RaMS
  - Did the PCA things, sometimes successful

In a Bayesian framework, the base rate (likelihood of being a real peak) is critically important for any downstream analysis.


## Supplement

Literally ALL the confusion matrices (as a table?)

Include data from other classifiers
  - Clustering(?)
  - Random forest
  - Lasso/ridge/elastic net regression
  
Figure: feature selection / model selection barplot from powerpoint
  - Too many numbers for the main text but useful for power users
  - Colored histograms (Results Fig 1) but for other param options
  - Red/gold barplot at 0.1, 1, 50, 99, 99.9 thresholds

Figure: model stability under smaller training set?
  - Both for all params and raw data params?

Figure: model estimate stability errorbar plot?
